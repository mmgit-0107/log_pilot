import os
import sys
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings

# Set Env Var for Local Testing
os.environ["LLM_BASE_URL"] = "http://localhost:11434/v1"

def test_ollama():
    print("ü§ñ Testing Ollama Connection...")
    try:
        llm = ChatOllama(model="llama3", base_url="http://localhost:11434")
        resp = llm.invoke("Hello, are you there?")
        print(f"‚úÖ LLM Response: {resp.content}")
    except Exception as e:
        print(f"‚ùå LLM Connection Failed: {e}")

    print("\nüß† Testing Embeddings...")
    try:
        embeddings = OllamaEmbeddings(model="llama3", base_url="http://localhost:11434")
        vec = embeddings.embed_query("Hello world")
        print(f"‚úÖ Embedding generated (dim: {len(vec)})")
    except Exception as e:
        print(f"‚ùå Embeddings Failed: {e}")

def test_ragas_import():
    print("\nüìö Testing Ragas Import...")
    try:
        from ragas import evaluate
        from ragas.metrics import faithfulness, answer_relevancy
        print("‚úÖ Ragas imported successfully.")
    except ImportError as e:
        print(f"‚ùå Ragas Import Failed: {e}")
    except Exception as e:
        print(f"‚ùå Ragas Error: {e}")

if __name__ == "__main__":
    test_ollama()
    test_ragas_import()
